---
id: schedule
name: Schedule
heading: Course schedule
subheading: 
image: 
---

<table class="table table-condensed">
	<tbody>
		<tr>
			<th>Week</th>
			<th>Topic</th>
			<th>Material</th>
			<th>Assignments</th>
		</tr>
			<tr>
				<td>Feb 7</td>
				<td>1. Introduction</td>
				<td>
					Brief Introduction to ML (<a href= "introduction_ml.pdf">slides</a>)<br>
					<a href= "http://videolectures.net/bootcamp07_keller_bss/">Linear Algebra and Probability Review</a> (part 1 Linear Algebra, part 2 Probability)
				</td>
				<td>
				<a href= "assign1.pdf">Assignment 1</a>
				</td>
			</tr>
			<tr>
				<td>Feb 14</td>
				<td>2.1 Bayesian decision theory</td>
				<td>
					[Alp10] Chap 3 (<a href= "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/2e_v1-0/i2ml2e-chap3-v1-0.pdf">slides</a>)<br>
				</td>
				<td>
				</td>
			</tr>
			<tr>
				<td>Feb 21</td>
				<td>2.2 Estimation</td>
				<td>
					[Alp10] Chap 4 (<a href= "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/2e_v1-0/i2ml2e-chap4-v1-0.pdf">slides</a>)<br>
					Bias and variance (<a href= "http://nbviewer.ipython.org/6788818">IPython notebook</a>)<br>
				</td>
				<td>
				<a href= "assign2.pdf">Assignment 2</a>
				</td>
			</tr>
			<tr>
				<td>Feb 28</td>
				<td>2.3 Linear models</td>
				<td>
					[Alp10] Chap 10 (<a href= "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/2e_v1-0/i2ml2e-chap10-v1-0.pdf">slides</a>)<br>
				</td>
				<td>
				</td>
			</tr>
			<tr>
				<td>Mar 7</td>
				<td>3.2 Kernel methods</td>
				<td>
					Introduction to kernel methods (<a href= "https://fagonzalezo.github.io/ml-2016-2/kernels.pdf">slides</a>)<br>
					[Alp10] Chap 13 (<a href= "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/2e_v1-0/i2ml2e-chap13-v1-0.pdf">slides</a>)<br>
				</td>
				<td>
				</td>
			</tr>
			<tr>
				<td>Mar 14</td>
				<td>4.1 Support vector learning</td>
				<td>
					[Alp10] Chap 13 (<a href= "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/2e_v1-0/i2ml2e-chap13-v1-0.pdf">slides</a>)<br>
					<a href="http://axiom.anu.edu.au/%7Edaa/courses/GSAC6017/tekbac_4.pdf">An
						introduction to ML</a>, Smola<br>
					<a href="http://www1.cs.columbia.edu/%7Ekathy/cs4701/documents/jason_svm_tutorial.pdf">Support
						Vector Machine Tutorial</a>, Weston<br>
				</td>
				<td>
				<a href= "assign3.pdf">Assignment 3</a>
				</td>
			</tr>
			<tr>
				<td>Mar 21</td>
				<td>4.3. Neural network learning </td>
				<td>
					[Alp10] Chap 11 (<a href= "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/2e_v1-0/i2ml2e-chap11-v1-0.pdf">slides</a>)<br>
					Quick and dirty introduction to neural networks (<a href= "https://gist.github.com/fagonzalezo/c1f56629890dcf5670aa">IPython notebook</a>)<br>
					<a href= "backpropagation.pdf">Backpropagation derivation handout</a>
				</td>
				<td>
				</td>
			</tr>
			<tr>
				<td>
				  Apr 4-18
				</td>
				<td>
				3.3 Representation learning <br>
				4.4 Deep learning <br>
				</td>
				<td>
					Representation Learning and Deep Learning (<a href= "https://github.com/fagonzalezo/dl_tutorial_upv/raw/gh-pages/UPV-dl.pdf">slides</a>)<br>
					<a href= "https://fagonzalezo.github.io/dl_tutorial_upv/">Representation Learning and Deep Learning Tutorial</a> <br>
					<a href= "https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/Handout-CNN-sentence-classification.ipynb">CNN for text classification handout</a> <br>
					<a href= "https://github.com/fagonzalezo/dl-tau-2017-2/blob/master/Handout-LSTM-language-model.ipynb">LSTM language model handout</a> <br>
				</td>
				<td>
				<a href= "https://github.com/fagonzalezo/ml-2018-1/blob/master/assign4.ipynb">Assignment 4</a>
				</td>
			</tr>
			<tr>
				<td>Apr 25</td>
				<td>4.2 Random forest learning</td>
				<td>
					[HTF09] Chap 15 (<a href= "http://statweb.stanford.edu/~tibs/ElemStatLearn/">book</a>)<br>
					Random Forest and Boosting, Trevor Hastie (<a href= "http://www.slideshare.net/0xdata/gbm-27891077">slides</a>)<br>
					Trees and Random Forest, Markus Kalisch (<a href= "https://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.1.pdf">slides1</a>, <a href= "https://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf">slides2</a>)<br>
				</td>
				<td>
				</td>
			</tr>
			<tr>
				<td>May 2</td>
				<td>5.1 Mixture densities </td>
				<td>
					[Alp10] Chap 7 (<a href= "http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/2e_v1-0/i2ml2e-chap7-v1-0.pdf">slides</a>)<br>
				</td>
				<td>
				</td>
			</tr>
			<tr>
				<td>May 9</td>
				<td>5.2 Latent topic models<br>5.3 Matrix factorization </td>
				<td>
					Latent Semantic Analysis, CS158 Pomona College (<a href= "http://www.cs.pomona.edu/classes/cs158/resources/158-12(LSA).pdf">slides</a>)<br>
					Latent Semantic Variable Models, Thomas Hofmann (<a href= "http://videolectures.net/slsfs05_hofmann_lsvm/">videolecture</a>)<br>
					Non-negative Matrix Factorization for Multimodal Image Retrieval, Fabio Gonz√°lez (<a href= "https://fagonzalezo.github.io/ml-2016-2/NMF-MM-IR.pdf">slides</a>)<br>
				</td>
				<td>
				</td>
			</tr>
			</tbody>
		</table>
